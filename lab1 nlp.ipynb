{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3994e2e3-9d9d-49c2-a241-fda7778de79d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### installing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a1429c-7fa0-48b5-b0a2-a79f0f1dbc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\kaush\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.7.34-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kaush\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 23.0 MB/s eta 0:00:00\n",
      "Downloading regex-2025.7.34-cp313-cp313-win_amd64.whl (275 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------- ----------------------------- 1/4 [regex]\n",
      "   ---------- ----------------------------- 1/4 [regex]\n",
      "   ---------- ----------------------------- 1/4 [regex]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ---------------------------------------- 4/4 [nltk]\n",
      "\n",
      "Successfully installed click-8.2.1 nltk-3.9.1 regex-2025.7.34 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc9aff6f-abaf-488c-beba-052df10a6e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaab99c-0798-4dcc-b89c-f81e769dc951",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### tokenization of sentences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ddac5-3769-4430-aef6-26021679477b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "934676b9-01f6-4247-bacb-6069c8c7ec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing is fascinating.', 'Tokenization makes text easy to process!', '.', 'this is my third sentence.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Natural Language Processing is fascinating. Tokenization makes text easy to process! . this is my third sentence.\"\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bf3bed7-b67b-42f0-bc19-f43bf3eb6e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing is fascinating.\n"
     ]
    }
   ],
   "source": [
    "# printing 1st sentence\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dc5f23f-d7d7-48fb-ae43-e36d27c32ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization makes text easy to process!\n"
     ]
    }
   ],
   "source": [
    "# printing  2nd sentence\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "244463f5-3ec9-4f3c-b449-934291b3165a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "# printing 3rd sentence\n",
    "print(sentences[2]) ## here we can see that even a single dot is considered as a sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9642d77d-5261-49dc-8adf-d7f774d4fae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is my third sentence.\n"
     ]
    }
   ],
   "source": [
    "# printing 4th  sentence\n",
    "print(sentences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a891a3ff-c51f-422b-8c2a-be07ba648321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing is fascinating.', 'Tokenization makes text easy to process!', 'this is my third sentence.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Natural Language Processing is fascinating. Tokenization makes text easy to process! this is my third sentence.\"\n",
    "\n",
    "# Sentence tokenization\n",
    "s  = sent_tokenize(text)\n",
    "print(s )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b7efa4f-1a1a-4a4c-80e1-eefbfbd07c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is my third sentence.\n"
     ]
    }
   ],
   "source": [
    "print(s[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360526e4-2ef7-44a4-b4d2-615e4e412197",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### tokenization of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0da54947-bffe-4931-ac28-0cd3fdc77ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'fascinating', '.', 'Tokenization', 'makes', 'text', 'easy', 'to', 'process', '!', '.', 'this', 'is', 'my', 'third', 'sentence', '.', 'and', 'i', 'want', 'to', 'tokenize', 'my', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Natural Language Processing is fascinating. Tokenization makes text easy to process! . this is my third sentence. and i want to tokenize my words.\"\n",
    "\n",
    "# Word tokenization\n",
    "words = word_tokenize(text)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45c471b0-23b7-4ccd-9bf1-5c5159adcc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation marks (like ., !) are returned as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ffa70ec-6d5e-49a1-8e34-c3d0c8304cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word: Natural\n",
      "Fifth word: fascinating\n",
      "First three words: ['Natural', 'Language', 'Processing']\n"
     ]
    }
   ],
   "source": [
    "print(\"First word:\", words[0])\n",
    "print(\"Fifth word:\", words[4])\n",
    "print(\"First three words:\", words[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f82b7-ea89-4d82-b77f-a50800888954",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### tokenization with gutenber corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7606b2d9-9505-4b54-81bb-435bcf6dffc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\kaush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download ('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdf3f102-cc2f-45e8-8296-9eca0803ea81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "print(gutenberg.fileids()) # List available texts\n",
    "text = gutenberg.raw('bible-kjv.txt') ## bible choosen from the gutenberg library \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "426be27c-2267-41be-85f0-6becc651f5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 sentences: ['[The King James Bible]\\n\\nThe Old Testament of the King James Bible\\n\\nThe First Book of Moses:  Called Genesis\\n\\n\\n1:1 In the beginning God created the heaven and the earth.', '1:2 And the earth was without form, and void; and darkness was upon\\nthe face of the deep.', 'And the Spirit of God moved upon the face of the\\nwaters.', '1:3 And God said, Let there be light: and there was light.', '1:4 And God saw the light, that it was good: and God divided the light\\nfrom the darkness.']\n"
     ]
    }
   ],
   "source": [
    "# doing sentecs tokenization \n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"First 5 sentences:\", sentences[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7748aea7-3989-45fb-a9ea-66089affd942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 word tokens: ['[', 'The', 'King', 'James', 'Bible', ']', 'The', 'Old', 'Testament', 'of']\n"
     ]
    }
   ],
   "source": [
    "# doing word tokenizaton \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(text)\n",
    "print(\"First 10 word tokens:\", words[:10])\n",
    "## we can clearly see that word tokenization doesn't include line spaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfcb504f-32c6-4ab4-8692-c259cf295e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "[The King James Bible]\n",
      "\n",
      "The Old Testament of the King James Bible\n",
      "\n",
      "The First Book of Moses:  Called Genesis\n",
      "\n",
      "\n",
      "1:1 In the beginning God created the heaven and the earth.\n",
      "\n",
      "1:2 And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep. And the Spirit of God moved upon the face of the\n",
      "waters.\n",
      "\n",
      "1:3 And God said, Let there be light: and there was light.\n",
      "\n",
      "1:4 And God saw the light, that it was good: and God divided the light\n",
      "from the darkness.\n",
      "\n",
      "1:5 And God called the light Da\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Loading the raw Bible text\n",
    "raw_bible = gutenberg.raw('bible-kjv.txt')\n",
    "\n",
    "print(type(raw_bible))         \n",
    "print(raw_bible[:500])          \n",
    "#print(raw_bible)               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f5ac128-09a0-4320-ac75-d40d95cc332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word tokens: 947008\n",
      "First 20 tokens: ['[', 'The', 'King', 'James', 'Bible', ']', 'The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible', 'The', 'First', 'Book', 'of', 'Moses', ':']\n"
     ]
    }
   ],
   "source": [
    "# further tokenizing the sentences in words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(raw_bible)\n",
    "\n",
    "print(\"Total word tokens:\", len(words))\n",
    " \n",
    "print(\"First 20 tokens:\", words[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c33fd70b-aee2-4f48-b646-491d9ebe96ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words: [(',', 70573), ('the', 62103), ('and', 38847), ('of', 34480), ('.', 26202), ('to', 13396), ('And', 12846), (':', 12706), ('that', 12576), ('in', 12331)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(words)\n",
    "print(\"Most common words:\", word_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb40d11-de4d-4ebb-b41c-ad9898970f6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### regular expression for tokenziign text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8516bc2e-98c0-48ba-a403-94d3c7f7447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Geeks', 'for', 'Geeks']\n"
     ]
    }
   ],
   "source": [
    "#split() fucntion\n",
    "import re\n",
    "text = \"Geeks,for,Geeks\"\n",
    "result = re.split(r',', text)\n",
    "print(result)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "050e58ff-383e-43f7-8964-77eb3bc67af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['geeks', 'for', 'geeks', 'gfg']\n"
     ]
    }
   ],
   "source": [
    "# splits where evr it sees brackets , commas collon and lll the give pattern \n",
    "import re\n",
    "s = \"geeks,for;geeks gfg\"\n",
    "result = re.split(r\"[,;\\s]+\", s)\n",
    "print(result)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d010eb9-06df-47f6-b442-5d3190c30f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', 'two', 'three four']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'\\s+', 'one two three four', maxsplit=2) # maxsplit allows the provided split in the give sentenxe \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa15674b-1e70-4c8e-bcbe-fc4409edd823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'How', 'are', 'you', '']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization of non words \n",
    "re.split(r'\\W+', 'Hello! How are you?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c177122-6ece-49f1-8997-cd9705a2c40c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### NORMALIZATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9649404b-d257-4319-81dd-9f1ae78fbce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello this is nlp lab'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOWERCASING\n",
    "word = \"Hello THIS IS NLP LAB\"\n",
    "word.lower()          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca71de5e-5782-4f5b-9c6c-b895eee844eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is NLP.\n"
     ]
    }
   ],
   "source": [
    "# Removing Extra Whitespaces\n",
    "text = \"This   is   NLP. \"\n",
    "normalized = ' '.join(text.split())\n",
    "print(normalized)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4e4d7-1f8b-497c-a0ab-7b721dfd00a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
